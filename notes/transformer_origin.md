## Transformer

### 简介

transformer是nlp中带有变革性创新意义的翻译模型，是一种完全依赖于自注意机制的全新的网络结构，核心亮点是放缩的点乘注意力机制，多头注意力机制，长时注意力模型，每个词嵌入都含有其它词的信息，而且能够并行化处理。之前的seq2seq模型中也使用了注意力机制，但是都是在RNN或者是在LSTM上使用的，依然不能很好解决计算并行化问题。所以提出了一种简单的，基于点乘注意力机制的网络。

### 核心驱动

降低对时序类数据的计算量和计算速率。更好地捕获全局信息，长时间关联性。

### 网络构成

核心构成为两个部分，一个是编码器一个是解码器，两者的关键构建是相同的，编码器与解码器的核心不同之处在于，解码器需要mask机制，而且需要接收来自编码器的信息。

#### 编码器

编码器部分由6个完全相同的层构成，每一个层有两个子层，第一个是多头注意力机制，第二个就是ffn增加非线性，文中也称之为位置层面的ffn. 每一个子层都会使用全连接机制将其连接起来，后面跟着一个层归一化层，因此，每一个输出的层可以表示为：LayerNorm(x + Sublayer(x)). 词嵌入的维度是512，方便残差连接。

#### 解码器

和编码器的两个子层结构基本相同，不同的地方是编码器中间插入了一个多头注意力层，结合了编码器的输出。与编码器最不同的区别是，自注意部分修改原本的结构，改为使用带有掩码机制的多头注意力机制，防止信息泄露，即每一个输出的词只能依赖前面的词的信息。masking的核心作用。

#### 注意力部分

通俗理解，注意力机制可以描述为query与key-value对的映射问题，最终的输出是所有value的加权求和，

##### 自注意力机制（放缩点积注意力）

通过query和key的点积获得权重，需要注意的query和key拥有相同的维度值，为了保证最后的梯度不会爆炸，通过除以一个
$$
\sqrt[2]{dim}
$$
得到放缩之后的权重，然后通过Softmax函数计算得分，这些都是通过矩阵计算进行了，加速计算速度。
$$
Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt[2]{dim_k}})V \tag{1}
$$
如果不使用放缩，对于维度比较大的时候，导致点乘的值会呈指数级增长，从而使得softmax进入到一个产生梯度很小的区域。为了对抗这一点所以使用了放缩因子。

#### Multi-Head Attention机制

将Query,Key和Value线性投影到不同的子空间，学习多个不同的线性映射，所有的不同的组同时学习不同的注意力，产生多组输出。多头注意力的关键作用是使得模型能够在多个子空间产生注意力，这也解释了为什么后面要有一个线性连接层，将多个头信息重新整合。

无论是在前面的多个头的产生还是后面的头的合并都可以通过矩阵计算来提高计算速率。因为根据矩阵乘法的可拆分性，实际上就可以看作是一个大的矩阵相乘，然后在拆分开，和多个小矩阵分别相乘是一样的。

#### Transformer中的多种注意力

* 编码器-解码器注意力

  核心是在解码过程中，query来源于之前的解码器层，但key和value来源于编码器的输出。这样做的好处是让每一个解码器位置上的词能够获得需要翻译的句子的全部信息，这样做模拟了典型的编码器和解码器类型的注意力机制，例如seq2seq翻译模型。

* 编码器中的自注意力

  在编码器当中，自注意力的输入全是来自于同一个子空间，即上一层的输出，每个位置的嵌入都捕获了上一层所有位置的信息。

* 解码器中的自注意力

  解码器当中的自注意力机制关注于当前位置之前的所有已经输出的词，防止信息泄露出去，所以需要mask那些未来出现的词。具体做法是产生的注意力矩阵中，将上三角区域用负无穷代替，这样产生的权重就为0。

#### 前向传播层

原始论文中使用了两层线性层和一个ReLU激活层，增加非线性能力。同时全连接层是位置分离和独立的，所以又被称为Position-Wise Feed-Forward Networks. 原始模型中词嵌入的维度为512，中间隐层单元数目为2048.

#### 位置编码

transformer中的所有词嵌入都是并行处理的，为了处理时序信息，需要用位置编码来代替每一个词在整个句子中的位置.  位置编码相加只在第一个编码器层和解码器层，而且维度一致，所以可以直接相加。位置编码信息是可学习的，也可以是静态的。在transformer中采用了固定的编码方式。
$$
PE(pos,2i) = sin(pos/1000^\frac{2i}{d_model})
\\
PE(pos,2i + 1) = cos(pos/1000^\frac{2i}{d_model})  \tag2
$$
其中pos是当前词在整个句子中的位置，i是词嵌入的维度。实际上就会将所有的位置编码和正弦函数相关起来，根据和差化积公式，可以使得某一个位置可以通过其它位置线性表示。使用可以学习的位置信息和固定的位置信息在最终的结果上是一致的。

### 自注意力的意义

主要体现在三个方面：每一层的计算复杂度，计算的可并行化，最小的必须必要的操作数目，以及网络中存在的长时依赖。长时依赖问题是翻译类任务的核心关键挑战，影响这种能力的核心因素是前向传播和反向传播的信号遍历路径太长了，因为序列就很长，尤其是在RNN系列模型当中。同时含有位置信息的输入到最终的输出之间的路径越短，这种长时依赖更容易学习。

如果使用单个卷积层，核大小k<n并不能捕获到所有位置的信息，只能通过将log_k n个扩张卷积堆叠起来，卷积网络通常比循环网络资源耗费更多。而且实际上可分离的卷积能够降低计算复杂度达到(k·n·d + n·d^2), 这个复杂度实际上和自注意层是相等的。

#### 其它必要关键技巧

##### Dropout的使用

transformer中会随机dropout掉子层的输出，并且在残差连接和规范化之前进行，此外对于添加了位置信息的词嵌入，编码器和解码器都会进行dropout,根据代码实现，一般都会在三个地方进行dropout, 即词嵌入输入层，对词嵌入进行dropout，在dot-product层对得到的权重进行dropout，以及对子层的输出进行dropout（原始transfomer会在两个子层都进行dropout)。对于较大的数据集和模型使用较大的值。

使用adam优化器，规范化层中使用较小的eps. 其中β1 = 0.9， β2 = 0.98，eps = 1e-9, 学习率动态调度：带有热重启的学习率更新方法。

使用平滑的Label Smoothing.





