## Deformable DETR

### 简介

DETR方法优化速度太慢了，而且特征空间尺寸太小。那些注意力机制仅仅关注于某个索引的部分key采样集合。模型结合了deformable convolution的计算高效性、稀疏空间采样和transformer的长时关系建模。

#### 高效注意力机制

* 在keys上使用稀疏的注意力机制，最直接的方法就是使用静态滑窗限制注意力模式（也就是自己想的那种采用局部平均或者最大值的情况），但是会丧失全局信息。补救的方法之一就是使得一部分特殊的tokens能够访问到所有的元素。
* 第二种是学习数据依赖的稀疏注意力。例如使用哈希机制将query和key映射为不同的bins。或者使用k-means寻找最相关的元素。
* 第三种则是寻求自注意力机制的低值特性，例如通过线性投影降低key的维度。或者核化自注意力操作。

而在图像领域，依然停留在第一种方法。就单纯的注意力机制而言，可形变卷积操纵比transformer中的自注意力机制更高效且在图像识别上表现良好，但是缺少元素之间的相关性联系。

### 核心思想

认为将transformer应用在图像领域的最大问题是transformer会在所有的空间点上进行计算。因此采用deformable convolution的思想，仅仅在与一个相关点对应的比较小的key集合上进行，而不是所有空间尺寸上。

### 方法模型

### 训练细节

