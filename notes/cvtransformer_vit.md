## ViT

### 核心思想

CNN中的局部平移不变性和局部性使其不能学习到长时依赖信息和全局信息，将transformer直接应用到视觉当中，尽可能做少量修改。而且需要考虑到计算效率和参数量。

Transformer中缺少CNN中那种偏置归纳能力，需要大量的数据来训练。模型的输入可以直接是像素级别的原始输入，也可以是经过卷积之后的特征映射压扁之后的向量。

### 模型结构

采用的基本结构是transformer中的编码器结构，尽量不做修改。同时为了进行分类任务，同样增加了一个可学习的cls_token来作为分类依据。位置编码那一块使用的是可学习的位置编码。

包含的主要结构有多头注意力，前向ffn，位置编码，残差连接，以及层归一化。使用GELU非线性激活。

### 训练细节

在resnet系列网络上进行训练，使用Adam优化器，β1 = 0.9， β2 = 0.999，使用巨大的batch size: 4096和很高的权重衰减0.1； 实验发现Adam优化器比SGD效果更好，使用SGD优化器的时候其batch_size为512。

