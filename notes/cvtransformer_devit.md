# DEIT

## 简介

想实现一种完全基于自注意机制的视觉transformer， 可以认为是ViT的扩展版本，而且使用teacher-student机制进行知识蒸馏。

## 核心思想

就是将transformer机制和知识蒸馏结合起来，达到比较好的结果。

## 方法模型

### 模型结构

1. 基本的多头注意力机制，和原始transformer基本类似。
2. 使用GELU为激活函数的FFN层，和原始transformer基本类似。其它地方有所不同的是在输入图片的映射上还是使用了ViT中的线性映射。
3. 使用了cls_token机制。在输入前添加到每一个样本上，最后提取出来进行分类，这样可以将所有的知识传播到cls_token上面。
4. 使用和分辨率相关的位置编码。

### 注意力蒸馏

假设已经获得了一个分类效果比较好的强分类模型，现在转变为如何从这个强teacher中学习知识。这包括了两个方面： 和软注意力相对的硬注意力机制，和token蒸馏相对的类别蒸馏。

#### 软蒸馏

通过最小化teacher和student的softmax之间的KL散度。文中又添加了一个distillation token，和cls_token分别放在首尾。所有的tokens产生都是一样的，唯一的不同作用之处在于输出的使用上，dis_token可以用来重新产生teacher预测到的硬标签而不是真正的标签。

全局的分类损失就可以定义为：分类损失和输出分类logits和teacher的分类logits之间的KL散度，然后再通过权重因子权衡。其实这就是一种软蒸馏方式。

#### 硬蒸馏

即不以真实的标签作为最终的衡量对象，而是以teacher的输出直接作为真正的标签，即teacher的logits的最大值即代表了真正的标签。二者糅合起来就变成了真正标签的交叉熵损失和硬蒸馏的交叉熵损失。

通过在输入嵌入中加入了cls_token和dis_token来做到对分类和知识的蒸馏，而它们之间是一直按照self-attention机制交互的。发现这两者的余弦距离会越来越近，最后会变得很高但始终不相等。而如果是加入了两个cls_token，实际上它们最终几乎会变得一样，这也从侧面说明蒸馏让网络学到了新东西。

最终将蒸馏产生的 结果和分类产生的结果合并起来进行测试。

## 训练

使用了convnets作为teacher, 已经预训练好，虽然teacher的精度可能不是很高，但是蒸馏出来的transformer方法表现的会比teacher好.





