## Lambda Networks

### 简介

在输入和结构化语义信息之间捕获长时的交互信息，具体做法是通过将可用语义信息转化为线性函数，然后将这个线性函数分别作用到每一个输入上来捕获长时交互。利用这种机制可以捕获全局或局部交互信息，同时和其它注意力机制相比计算量显著减小且能够应用到更长的输入序列上。关键的核心创新是将那些语义信息转换为线性函数，然利用线性函数可以极大简化计算量。在大规模数据集分类、目标识别、实例分割上都取得了最好的成绩。而且速度也比经典的残差网络更快。

### 核心思想

将keys看作捕获查询和他们对应语义的一种需求，lambda这种线性模型也可以是注意力机制一种可替代的方案。

### 模型方法

将查询和语义都看作是一些k维向量和序列坐标组成元组的一些集合，即Q = {(qn,n)}和C = {(cm,m)}。查询和语义序号坐标之间的位置对（n.m)可以看作是一种结构元素之间的相关性，例如可以代表二维图像上的距离等。

重新考虑在给定内容语义C以及线性函数F:((qn,n),c) -> yn将查询(qn,n)映射到yn. 这个线性函数显然能够当作一个神经单元层来处理结构化输入。（qm,cn)可以看作是基于内容语义的交互，（qn,(n,m))可以阚总是基于位置的交互。

当输出yn依赖于所有的(qn,cm)或者（qn,(n,m))交互时，它们并且是局部的，仅当n周围很小的一块语义被考虑的时候，这个时候就可以说F是捕获了全局的交互信息。最终，这些交互可以被定义为密集的，如果他们包含了所有的语义元素。

#### 引入捕获长时交互的Keys

依然使用点乘机制来获取交互，基于内容的交互（qn,cm)需要一个基于cm的的k维向量，相反，基于位置的交互（qn，(n,m))需要一个k维的位置嵌入enm, 所有的这些维度都需要一同收缩合约到输出里面。每一个层捕获长时的交互被刻画为基于先联合查询的深度还是语义位置。

#### 注意力交互

收缩查询的深度首先会像普通的注意力操作一样在查询和语义元素之间创作一个相似性核，即注意力得分映射。当输入的序列长度过长，而query和输出的维度几乎一致的时候，这种计算可能看起来会有一些浪费。

#### Lambda interactions

取而代之的是，对于一个query, 通过一个线性函数 yn = F((qn,n),C) = λ(C,n)(qn)来简化注意力映射。在这种设定下，语义信息就会聚合到固定尺寸的线性函数λn = λ(C，n).  即每一个query都会有一个λ小的线性函数，依存于当前的语义，即根据当前的内容语义产生，然后在计算完毕产生query输出之后就会别丢弃。整个过程类似于早期的过程性编程，因此称之为lambda函数^_^.

#### lambda层

一个lambda层的输入是X和语义C，并且产生线性lambda函数来生成输出Y, 当C = X的情况下，就编程了自注意机制，它们的通道数都可以是一样的。它可以支持产生长时密集的基于位置的非具体化注意力映射图的内容和交互。

#### lambda函数生成

lambda层首先通过线性层操作将内容映射为key和values. 然后keys就会沿着语义位置，通过softman操作产生一个规范化之后的keys, 这个过程可以看作是一种函数式的信息传递，对于每一个内容元素都能贡献一个内容函数和一个位置函数。而λn就是将所有内容贡献都加起来，得到一个k x v的矩阵。

总而言之，λ函数的产生过程首先是线性映射，得到key和value，然后对key沿着内容位置方向进行softmax规范化，然后和相同位置的value相乘，得到一个矩阵，将所有的加起来就得到一个关于内容的λ函数，基座λc, 同理也能产生关于位置的函数，将两个函数加起来就得到了这一层的λ函数。

其中，内容函数是不变性的，即对所有的query都进行同样的操作，相反，位置函数编码了如何根据内容和位置来转换某一个固定位置的查询。然后对于每一个query, 就可以使用合约之后的λ函数。

#### 关于lambda函数的解释

对于最终得到的λ函数，实际上就是一个二维矩阵，可以被视作一个固定大小的kv语义特征，这些语义特征从语义内容和结构中聚合而来。利用这个矩阵就能过促使语义特征产生合适的输出，从而捕获长时密集的交互。

使用batch-normalization does better.

提出使用lambda卷积来捕获局部的归纳偏置性，这对于很多任务来讲都是很有帮助的。因而可以对位置编码产生lambda, 考虑到局部性，可以将一维的编码变为多维网格，从而使用二维卷积达到局部计算目的。

### 复杂度分析

---- 更低的时间复杂度。

### 训练策略

无。

#### 简单总结

   对于自注意力中query,key,value的理解。

​    和自注意机制中并不相同，自注意机制当中是通过点积的形式构建相似性核，计算每一个query与其它key的相似性，得到相似性得分，然后根据相似性得分将

​    其它的向量加权求和得到新的向量，实际上本质就是一种将一个向量用其它向量线性表示，从而融合每一个向量的独有特征，达到每一个向量都融合有其它向量的信息。

​    整个过程依赖于点积作为相似性核，而且会将原始输入映射到三个不同的子空间从而表达不同的关注点，即想要表达的语义需求query, 向量的本质特征key, 向量已经含有

​    的实际上下文语义value(这一点自己理解的)，这三个实际上是很难有界限划分的，只能人为这样设定，从它们来自同一个输入就能看出。

​    点积相似性核其实计算量是比较大的，时间复杂度为2次方，对于比较长的序列计算成本会比较高。因此提出需要降低计算成本达到线性。



​    

​    lambda中也存在相关的概念，只不过使用思想有所不同。

​    query依然可以理解为想要的语义表达需求，只不过是通过语义注意力或交互与位置注意力或交互得到。key则为查询的特征，其中|k|代表了查询的深度，value表示查询到的值。

​    其中很重要的一点是：

​    \1. query和key查询时的维度是相同的，然而value的维度可以有所不同，这一点与自注意力机制基本相同。

​    \2. lambda中依然存在多头机制，只不过这个多头存在的意义是权衡计算量与性能(瓶颈在于比较的|k|)。表现在内容的注意力key是共享的，最后的输出会分为多个通道然后合并。

​    \3. 内容函数是共享的，然而还考虑到了相对位置信息，这个是和查询具体的位置相关的。最终的输出包括了内容和位置，线性求和。

​    \4. 使用lambda卷积完成全局交互的捕获和局部的捕获，利用爱因斯坦表达式快速完成计算。



​    有一点非常重要，需要转换切入点，在self-attention机制中重点在query和key的自注意力机制，而在lambda中重点在于线性函数lambda的构建，切入点不同导致

​    思考的方向机会不同。

​    在自注意模型中，是根据query和key的相似程度去选择value的，而在lambda中，会先将key与全局语义进行矩阵乘法，然后相加融合，作为共享的语义函数，供所有的

​    query使用，这一点是降低计算量的关键，这一步操作可以看作是融合了所有的语义信息，还可以加入相对位置结构信息。



​    位置嵌入信息也是一个线性函数，只不过它的参数之一是一个可以学习的参数，记录了每个query位置对应的k-key与抽象元素u之间的相对位置嵌入，从而表示结构信息。

​    而且是和v相关的。

​    局部结构信息需要使用三维卷积完成，卷积核即为相对位置嵌入[k,u,1,m,m],全局信息则直接为一个[m,m]



​    理解的重难点: 为什么会有那个u-intra-depth出现？

​    文中始终没有说明为什么会有那个intra-depth,个人理解依然是抽象的内容语义的层数，用来增强表达能力。

#### LambdaResnet

将所有的残差结构换成lambda层即可。