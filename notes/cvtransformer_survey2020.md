# 视觉Transformer综述（2020）

## 简介

视觉自注意机制在各个层级的任务中都有出现，主要包括的领域范围有基础图像分类，视频处理等等。在视觉领域的处理主要包括了对自注意力机制的使用和对计算效率的提升。

Vision Transformer在低层任务（图像特征的表示和描述），基础分类任务，中级任务（识别、分割等），以及视频处理中都有应用。

## 分类任务

### iGPT

和GPT语言处理任务类似，可以看作是视觉版本的通用处理任务，分为两个阶段，一个是预训练阶段，即像预测词token一样来预测像素，训练好之后进行下游任务的训练。上面的是可以看作为无监督训练。

值得注意的是，在训练阶段，最后会通过一个线性层来将每一个sequence 映射出去得到分类结果。`在处理分类之类的下游任务时，会将得到的最后输出序列按照序列方向进行求平均池化得到一个d维度的向量，然后再进行最终的处理`。

### ViT

声称完全去掉了卷积操作（开始的那个线性投影依然可以看作是卷积的变种），通过大量图片进行训练，对于每一张图片都采用了位置编码和cls_token。只使用了encoder结构。使用了JFT300M大规模数据集，训练使用的时间大幅增长。

## 目标检测

利用transformer结构的目标检测模型可以大致分为neck-based、head-based以及framework-based几种。

首先是利用transformer对多个尺度的图像进行融合，即neck-based. FPT的多个尺度以及多种类型的transformer结构。

BVR则是通过多头注意力机制结合了多种特征融合为一个表示，而且将其中的主要的哪一个表示作为query，其它的作为auxiliary特征来增强主要的表示。即会有多个头用来检测。

#### DETR

DETR将目标检测问题看作是一个集合预测问题，而且避免了很多的利用先验手工设计组件，例如NMS和锚框生成之类的先验处理操作。处理流程方面，DETR首先会从CNN中抽取特征，然后借助Transformer机制完成特征和交互的捕获。

同样，先将特征压扁后加上位置编码，然后就是标准的transformer结构。DETR的核心点是在解码器当中，将需要预测的对象编码当作query送入解码器，从而获得各个位置的信息，然后再进行最后的分类任务。而且一般情况下可学习query的数目要比图像中的对象数目多一些。最后通过一个FFN获得坐标和类别的输出。使用二部图损失。最终取得了和Faster-RCNN类似的结果。

DeformableDETR主要考虑的是关键点附近的一个比较小的集合而不是所有的空间点位置，而且更容易融合不同尺度的特征，与此对应的是计算量会显著减小。









