# 批标准化思想及原理

## 简介

训练神经网络的核心困难之一就是每层输入的分布都会随着前面层参数的改变而改变，为解决这一点，就需要很好的参数初始化和比较小的学习率，这既增加了设计模型的难度也使模型收敛的速度变得缓慢，而且在面对饱和性激活函数的时候训练会变得更加困难。这种线性可以被称之为`内部方差偏移`，核心解决方案就是对每一层输入进行规范化或标准化。

BN可以允许使用比较大的学习率并且并不需要经过精心设计的网络初始化，而且还可以当作一个正则化方案来替代Dropout机制。

## 核心思想

随机梯度下降是非常简单和高效的优化方法，然而它却需要非常仔细地设置初始化参数和其它超参数，尤其是初始化权重和学习率，因为后面的网络都会很复杂地受到前面网络的影响，前面网络有一点偏差就会严重影响后面的网络。每一层输入的分布不断变化意味着网络需要不断地适应新分布，在一般的学习系统里面这个便称作是网络遭遇了方差偏移，这在神经网络中会一直存在。

如果所有子层的数据分布不随时间的变化而变化，那么所有子层就不会随着分布的变化重新调整，所以BN的核心思想是想让各个层的输入数据分布变得稳定下来，这不仅仅对处理的相关子层有帮助，对于其它子层外的那些网络部分也会有很好的帮助。尤其是对于后面的其它层以及外部网络部分。

## 方法

核心工作就是削减`内部方差偏移`来加速网络训练，即通过一个规范化操作或标准化操作来修改每一层输入的均值和方差。**对于梯度数据流参数更新方面也有很好的帮助，就是降低梯度对参数尺度或它们的初始值之间的依赖性**。通过在训练过程中不断修正网络子层数据的分布来加速训练。

### 一般的白化方法

可以考虑直接修改激活后输入的值或者是参数来保持数据分布的一致性。。如果考虑在每一个训练步可以白化它的激活值或者是一些内部值，或者直接修改优化器的参数来决定激活输出，这种修改方式和参数梯度更新是以交织的形式进行，那么梯度更新就会需要修改先进行，这会降低更新的效果。原因是偏置b的更新和规范化的改变不会带来任何影响。b会一直增长而损失却不变，当规范化参数在梯度更新步骤之后计算进行的时候就会使得模型崩溃。

如果要考虑到整个数据集合的数据特性，在反向传播的过程中会涉及到计算整个数据集与单层样本向量的雅可比矩阵，其中会计算协方差矩阵，计算成本高。

不同于一般的方法，像通过利用一整批训练数据的统计特性来白化一个样本从而保留网络中的统计信息。

## 细节

考虑到整体层输入白化的计算成本，并且并非处处可导，因此做了必要的化简，即将所有的每个标量特征各自独立地规范化，通过使其含有0均值和1方差，对于一个含有d维的输入而言，会通过统计整个批次中该维度的期望和方差来进行规范化。但这种操作显然会改变网络的表示，例如对于sigmoid激活就会限制它们退化为线性方式。因此需要确保网络中特征在转换前后能被恒等表达，这点又通过为每一个维度引入两个统计量：γ和β来缩放和平移规范化后的值。
$$
\hat x^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[{x^{(k)}}]}}. \tag1
$$

$$
y^{(k)} = γ^{(k)}\hat x^{(k)} + β^{(k)}. \tag2
$$
显然，如果缩放因子等于方差，偏移量等于期望，那么就会完全恢复至原始版表示。

第二个化简就是在小批量上进行而不是在整个数据集上计算统计量，每一个小批量数据中的统计量实际上可以近似估计整个数据集上的统计量，而且是在每个通道上独立进行。因此，整个规范化操作可以看作：对于一个mini-batch上，B = {x1...m}, 需要学习的参数是γ和β。
$$
μ_B \leftarrow \frac{1}{m}\sum_{i=i}^mx_i \\
σ^2_B \leftarrow \frac{1}{m}\sum_{i=i}^m{(x_i-μ_B)^2} \\
\hat x_i \leftarrow \frac{x_i - μ_B}{\sqrt{μ_B^2 + \epsilon}} \\
y_i = \gamma \hat x_i + \beta
$$
整个过程操作依然是可以求导的。（可以经常练习反向求梯度推导），上述是一般的通式，对于卷积操作，则是以通道为维度，计算各个通道的统计值。

无论是一般的全连接层还是卷积层，都是将BN操作加载激活层的前面，也可以对激活后的值进行规范化，也可以仅仅对计算前的输入x进行规范化，但是考虑到u很可能是另一个非线性层的输出，它的分布很可能在计算过程中发生变化，所以过早地进行规范化可能并不会估计方差偏移量。而对计算之后的值进行规范化，计算后的值很可能是对称而非稀疏的分布，然后再进行规范化操作会有一个更稳定的分布。考虑到充分利用卷积的特性，在卷积层的每一个通道使用一组相同的缩放和参数，统计所有样本中同一个通道数据上的统计数目。

## 其它

BN可以使用更大的学习率，它可以防止参数的微小变化放大为更大的或者次优的激活梯度. 例如它可以防止训练陷入非线性函数的饱和状态。这对于参数的尺度很有弹性，防止因学习率过高而导致的梯度爆炸。因为对于参数的线性尺度增长，对于雅可比矩阵的计算并不会有影响，而且通过求导可以看出，更大的参数产生的梯度反而更小。

BN本身也能起到很好的正则化作用，因为每一个样本都和其它样本关联起来，那么对于一个给定的样本而言，它就不会产生和样本相关的那些决定性值，从而防止过拟合。这样dropout机制就可以被移除一些或者强度不用设置那么高。

核心做法：确保分布的稳定性，加快训练。

