## DETR

### 简介

将目标检测任务看作是一个直接的集合预测问题，移除了那些手工设计步骤，包括锚框机制，NMS机制，给定一个静态的学习到的目标查询集合，直接输出目标的类别和坐标。不会再涉及到多个间接的处理阶段和损失设计。

在训练过程中，只使用一个集合二部匹配损失（预测集合、标注框）。最大的特点是：有其它集合预测相比，DETR是二部匹配损失与非自回归并行解码transformer的结合。

最终，在基于标准ResNet和transformer的实现下，对于在COCO之类的大型数据集，取得了和Faster RCNN差不多好的结果。此外，其它很多的任务都可以在DETR上进行，例如分割。不过，局限性体现在：

* 对于那些大的目标检测结果比较好，而那些比较小的目标效果比较差。
* 需要额外的训练模式，并且解码损失有很大的影响。

### 核心思想

指出transformer中的自注意力机制的核心之处在于显示地对一个序列中元素与元素对之间进行建模，这使得这种自注意力结构对于特定条件的，例如对去除冗余副本的集合预测任务就非常合适。

### 相关领域

#### 集合预测问题

基本的集合预测问题便是多标签分类，也是最基本的方法，这类问题的一个关键难点就是需要避免邻近副本问题，因此当前的方法主要使用NMS等手段来解决此类问题，此类方法是间接的，直接的集合预测是不需要后续处理的，而这种直接类方法的核心关键就是学习到各个待预测元素之间的关联信息。

对于常数级别的集合预测任务，全连接已经很充分了但是计算成本太高，例如使用自回归的RNN预测模型。对于集合预测问题，通用方案是设计一个二部匹配问题，保证枚举不变性，即每一个目标元素都有一个唯一的匹配。做法就是使用一个

#### Transformers和并行解码

从一个角度来讲，注意力机制即为神经层从一整个输入序列中整合信息，transformer机制其实和Non-Local 网络相同，扫描序列中的每一个元素并从整个序列中收集信息并更新它。

最早的transfomer模型是使用再自回归任务当中，例如机器翻译输出tokens, 而推理损失导致了并行生成任务的发展。因此，DETR将Transformer和并行解码任务结合起来。

#### 目标检测

主流的目标检测模型无论是单阶还是多阶最终效果都都严重依赖初始化设置，例如锚框机制等。

一些使用集合预测的方法需要和NMS结合使用。

### 模型方法

核心在于两点：

集合预测损失：建立集合直接预测和真实标注框之间的损失关系。

高效结构：预测一个潜在对象集合并且建立之间的关系模型。

#### DETR结构

分为三个部分，分别是特征抽取backbone，transformer编码器和解码器层以及预测输出层。

对于编码器：首先使用1X1卷积降维，压扁，每一个编码器结构都使用了标准的transformer结构，同时使用了静态的位置编码。将其添加到每一个注意力层。

对于解码器：同样遵循原始设计，即三层结构，自注意力层，编码器-解码器注意力层。不同之处在于在每一个解码器层并行解码N个目标。这些输入嵌入学习位置编码，并将其添加给每一个注意力层。N个目标查询就会被解码器转化为输出嵌入，然后通过前向网络，就可以得到box坐标和标签的N个预测。位置query是可学习的，大小也是指定的。

最终的输出通过三层MLP以及带有ReLU激活的网络组成，FFN预测归一化之后的坐标中心，宽和高，以及标签预测。在训练过程中使用了其它额外的损失。尤其是对模型对于每一个类输出正确数目的对象。

在每一个解码器层后面增添了FFN层和Hungarian损失，所有的预测层FFNs层共享权重。同时使用了额外的共享层规范化机制来规范化不同解码器的输出，同时也是FFNs的输入。

### 训练过程

使用了AdamW优化器，对于backbone部分权重设置为10-5，transformer部分使用10-4，权重衰减为10-4，使用了ResNet50和ResNet101作为backbone. 随机裁剪，然后放缩，transformer使用默认的0.1dropout。 

通过在最后一层增加dilation并移除掉一些一个步长，以及放大原始图像来增加分辨率。但是会增加自注意力的计算成本。训练了300轮，每200轮降低为十分之一。

### 其它

transformers一般都需要非常长的训练周期来训练，而且是使用那种Adam或者是梯度优化器之类进行更新，而不是SGD。

