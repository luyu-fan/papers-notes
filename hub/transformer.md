# Transformer笔记

## RNN存在的问题

处理序列数据时，RNN最大的问题在于每一个时间步都需要考虑之前时间步的隐藏状态，因此不能进行并行化。
尤其是在进行比较长的序列处理的时候，因此需要耗费比较多的时间。

## CNN存在的问题

使用CNN也可以进行序列化，而且也可以进行并行处理，但是一个很重要的问题时CNN的感受野是需要用层来进行叠加的，而且有
很强的偏好性，即每一个卷积核的权重中心即在他的感受野中心。虽然也可以捕获长时信息，但是只有在很高的层才能做到捕获
比较好的时间长度。

## Transformer

输入一个Sequence，利用自注意机制来并行处理词嵌入。对于每一条输入向量，首先将其通过全连接层映射为合适的向量嵌入。
然后在分别乘以三个不同的矩阵，分别得到Q,K,V三个不同的向量。

Query: 去匹配其它向量的。 qi = Wq * ai 矩阵乘法 
Key: 被用来匹配的，响应匹配Query。 ki = Wk * ai 矩阵乘法 
Value: 需要被抽取出来的知识。 vi = Wv * ai 矩阵乘法 
上述就能够利用矩阵乘法操作，并行化为每一条向量计算它的Q,K,V,这样就得到了Q,K,V矩阵。

现在用每一个Query去对Key做Attention,实际上就是做inner-product,计算他们的匹we配程度。具体使用的方法为：
Scaled Dot-Product Attention: a1,i = q1 · ki / d ^ (1/2)
因为所有的向量维度都是一样的，做了点积之后需要在除以他们的维度的平方根，这样做实际上是为了保证归一化结果。
这个分母的作用就是将上面分子的乘积方差变为1，做到归一化，防止梯度爆炸。

然后，使用softmax得到这个query与每一个key产生的权重，用概率的方式代表了与所有的key之间的相关程度。
然后，用这个概率分布向量去和V矩阵的所有向量相乘再相加，即加权求和，得到最后的利用这个Query抽取到的所有知识。
很大的特点就是这个知识向量的产生实际上已经用到了所有的其它向量的东西，即看到了所有的信息。换句话说，这个
向量就能够在很远的距离上捕获到和另外向量的关系，只需要有一个比较高的权重即可。

a ---> q,k,v ---> b: 中间就经过了一次Self-Attention Layer. 可以并行计算加速。

矩阵运算：
Q = WqA
K = WkA
V = WkA

中间的权重计算：
B = softmax(Q K^T / d^ 1/2) V

整个计算过程总结：
从Input到经过一层Self-Attention的Output,首先,这个输入矩阵会和三个矩阵相乘，分别得到Q,K,V,(以行为轴线) 然后使用QK计算就会得到权重矩阵A_hat。
利用权重矩阵A_hat就可以从V中抽取与Query对应的特征矩阵,也即为输出O.

### Multi-Head

得到了整体的Q,K,V之后,可以使用一定的方法产生多个子组Qi,Ki,Vi,这样就可以关注多个不同的部分。每一个子组只和对应的子组进行计算。
最终将结果拼接起来，如果维数过高，实际上还可以做一个降维，然后再进行下一次self-Attention。

### Position Encoding

对于Self-Attention来说，每一个向量实际上在文章中的位置是没有关系的，为了考虑真实的词与词之间的位置关系，还需要加上代表每一个词在句子中位置的
位置嵌入向量。本质上等于在原始输入上加一个指示位置的one-hot,再经过矩阵乘法得到最后的含有位置表示的矩阵，但这实际上和直接计算得到没有区别，因为
前者可以通过线性代数拆解相乘再相加。

### Seq2Seq Transformer

首先，对于自然语言的句子，会得到它的词嵌入，根据这个词嵌入再加上位置嵌入，就会得到Encoder的输入input.

input首先会经过之前的Multi-Head Attention模块，然后就会得到注意力之后的部分B,将其与之前的输入加起来，即残差连接，然后再进行层归一化得到下半部分的输入。
然后将上面的输出经过一个全连接层，再进行残差连接与层归一化，就会得到这个模块的输出。

## Transformer系列

基础的Transformer用来翻译，包括了两个部分，一个是Encoder，另外一个是Decoder, Decoder实际上里面还包括Mask部分，一种更复杂的结构。

### Sandwich Transformer

主要考虑的是交换里面的顺序，原始的Transformer中的顺序是先经过Self-Attention，然后经过全连接，中间还会加上以下残差层和归一化等，那么如果交互以下两者的顺序，
效果会不会好？
论文实验结果说的是将多个叠加的Transformer Encoder层中的注意力层和全连接层剥离出来，将注意力层放在前面，全连接层统一放在靠后的位置，这样会得到比较好的结果。
这样不会需要更多的参数以及更多的内存消耗，因为只是在改变结构而已。
sfsfsfsfsfsfsf ---> ssssssfsfsfsffffff: 将结构有所调整。声称效果有所提升，但是分布的很分散。结果实际上很随机。
只有那种前面的自注意层的数目比较多,后面的全连接层比较多的情况下效果比较好。论文中尝试的结果为前面为6的时候比较好。

### Universal Transformer

Transformer的特点是对于一般的翻译做的很好，但是对于一般的文本上的处理与演算能力比较差。比如复制文字，取代词语等。
主要的改进是在深度方向上进行重复，类似RNN。
同时还提出了一种Dynamic Halting机制，这样可以使得某些词语得到训练的次数是不一样的，即复杂度不一样。

### Residual Shuffle Exchange Network

主要的想法是，使用更少的参数，更少的时间复杂度。其中主要的机制就是shuffle和镜像操作。这样来捕获长时间向量之间的依存关系。
从而大大减少参数量以及显卡内存消耗。效果也不会太差。

## BERT系列

BERT实际上是一个语言模型，它是用Transformer的Encoder部分训练得到的。包括了两部分，语言模型的预训练，以及下游任务。
ALBERT --> 全部轻量化。使用循环模式结构。核心思想是减少参数。核心的思想是验证共享参数时最终的性能不会降低很多。

Reformer
多个Head注意力机制实际上关注的是不同的部分，每一个注意力机制实际关注的是比较少的。 将得到的Q,K,V矩阵拆分开多个头，分别进行自注意操作，最后的结果拼接在一起。
传统的Transformer结构的时间复杂度是O(n^2),如果序列很长，那么就会需要很长的时间进行计算。
Reformer的核心思想就是只对其中的关键子集进行自注意力操作，而不是对所有的进行。因此这里就借助了Hash操作进行划分，将相似度比较高的向量划分到一个子集里面。
只在相似度比较高的子集里面做Attention机制。

## Transformer中自注意力机制的理解以及在视觉中的使用


### 通俗理解

Transformer实际上声称捕获的是长距离的交互关系，这一点对于长距离建模是非常重要的，从而能够良好捕获全局信息，这一点对于CNN学习的是局部特性是完全不同的。

Transformer的核心是使得输入的每一个token都包含其它token的信息，从而每个词都含有长距离的关系。这一点是通过将词嵌入映射为三个矩阵，即QKV矩阵，通过点积的形式来匹配相关性。或者说计算相似度。然后在进行残差连接和层归一化，最后通过FFN增强非线性表达能力。

解码器与编码器相比轻微不同的地方在于它的Q是上一层的Q,，而K和V来自编码器的最后输出，此外还有mask掩码，这个覆盖掉那些未来的词，防止信息泄露。然后再通过线性层来对token进行分类，从而完成翻译任务。

### CV的特点或优点

卷积在从图像中提取特征时，能够提供合适的归纳偏置，实际上就是学到了那些知识。卷积运算有两个重要的空间特性：平移不变性和局部性。上述特点使得CNN计算比较高效，擅长提取视觉特征，但是无法对她们之间的依赖关系进行建模，为了捕获更大的感受野，就不得不进行建立更深的网络。

计算机视觉领域的自注意层以特征图为输入，目标时计算每一对特征之间的注意力权重，从而生成一个更新的特征图，其中每一个位置度有关于童话一图像中任何其它特征的信息，这些层可以直接替换或与卷积结合使用。这个就能够比常规卷积关注更大的接受域。因此能够建模空间上比较远的特征之间的依赖关系。

最基本的方法就是将特征压扁之后计算自注意力，但这个时计算昂贵的，对于高分辨率比较难以接受。

目前自注意机制主要体现在自身如何降低计算复杂度，提升效率上，从跨界角度考虑体现在如何将自注意机制应用到目标检测、分类以及其它生成类任务当中。

## Transformer系列论文

| 时间 | 会议 | 题目                                                        | 想解决的问题                 | 使用的方法                                                   | 结果                                                         | 训练细节                     | 其它                                                         |
| ---- | ---- | ----------------------------------------------------------- | ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------- | ------------------------------------------------------------ |
| 2017 | NIPS | [Attention is All you Need](../notes/transformer_origin.md) | 机器翻译，时序模型，长时模型 | Transformer, Encoder, Decoder, Self-Attention, Multi-head Attention, Position Encoding, Mask Encoding, Dropout, 标签平滑。 | 里程碑奠基作品，使得自注意力机制成为CNN,RNN之后的另一网络结构。 | Adam, GPUs，学习率动态热重启 | Transformer需要大量预料数据进行训练。在小规模数据集上表现不佳。 |
|      |      |                                                             |                              |                                                              |                                                              |                              |                                                              |

## Transformer in CV系列论文

| 时间 | 会议   | 题目                                                         | 想解决的问题                                                 | 使用的方法                                                   | 结果                                                         | 训练细节                                                     | 其它                                                         |
| ---- | ------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2018 | PMLR   | Image Transformer                                            | 对真实图像像素的建模，通过最大化对数似然，使得能够解决图像压缩之类的问题。生成更真实的图像。 | 将一个像素位置的点表示成三个255维度的向量，然后通过不同窗口大小和步长的卷积得到一个表示，作为最终的输入。<br />每一个像素位置还添加了一个像素序号的位置信息，使用一般的维度用来行编码，一半的维度用来列编码。<br />对于条件生成任务，使用编码器和解码器结构，编码器生成语义级别的，代表原始像素强度的表示，解码器在每个时间步回归生成每个像素通道的强度值。<br />原封不动使用原始的Transformer结构。 | ——                                                           | ——                                                           | ——                                                           |
| 2018 | PMLR   | Self-Attention Generative Adversarial Networks               | 针对图像生成任务，建立注意力驱动的、长时依赖的生成模型。充分利用了和对象形状相邻近的邻居区域而不是局部固定形状。<br />自注意力模块作为CNN的补充部分帮助建立长时依赖、以及多级别的图像区域。 | 核心要素就是在普通的CNN后面增加了一个Transformer层来捕获长时特征，自相关部分与原始transformer中的自注意力模块完全一致。<br />值得一提的是，它采用了压缩降维操作和可学习的残差连接权重。得到一个比较好的含有更多细节信息的特征。 | 比之前的工作更好，得到了巨大提升，提升了图像结果的感知得分。降低了距离。 | 使用4 GPUs训练了两周，生成128 * 128大小的图像，使用Adam优化器，判别器的初始学习率为0.0004，生成器的初始学习率为0.0001。 | 主要是将transformer机制引入到GAN当中，并没有做其它的太多的技巧。 |
| 2021 | ICLR   | [An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale](../notes/cvtransformer_vit.md) | 完全使用transformer机制进行图像分类，生成CNN的归纳偏置能力不好，应该使用transformer来获得更好的长时依赖和全局理解。 | 首先将图像分为若干个固定大小的patch，然后线性编码（本质上依然等同于卷积操作），然后使用transformer结构进行训练，结合位置编码，cls_token等机制。 | 经过大量数据训练的情况下，在几个大规模分类数据集的情况下取得了最好结果。 | Adam优化器，热重启、GPUs, GELU激活。                         | 在大规模数据集的情况下能取得比较好的成绩，但是在数据集比较小的情况下并不如CNN.但可以作为一个新的开端。 |
| 2020 | CVPR   | [DETR: End-to-End Object Detection with Transformers](../notes/cvtransformer_detr.md) | 将目标检测问题当作一个集合预测问题（二部图匹配问题）。直接通过集合预测输出而不是间接获取。摒弃了锚框、NMS之类的先验知识。 | 分为三个典型阶段，特征抽取，特征编解码，前向分类。编解码使用的是transformer中的标准结构，每一个解码器层后面跟着一个FFN. | 可以达到Faster-RCNN的性能，但对小目标检测较差。              | 使用Adam优化器。GPUs                                         | 提供了一种直接检测输出的模式。                               |
| 2020 | {CVPR} | [Deformable DETR: Deformable transformers for End-to-End Object Detection](../notes/cvtransformer_deformabledetr.md). | 认为将transformer应用在图像领域的最大问题是transformer会在所有的空间点上进行计算。因此采用deformable convolution的思想，仅仅在与一个相关点对应的比较小的key集合上进行，而不是所有空间尺寸上。 | 将注意力机制和deformable convolution结合起来。               | 降低计算量并且表现比DETR好。                                 | ——                                                           | ——                                                           |
| 2020 | ——     | [Learning Texture Transformer Network for Image Super-Resolution](../notes/cvtransformer_sr.md) | 利用transformer的注意力机制从Ref图像中获得transferred HR texture的RefSR方法，取得了最佳成绩。 | 主要分为了联合特征抽取，相关嵌入学习，特征融合，跨尺度整合多个模块。其中需要学习的是hard attention机制和soft attention机制。 | 取得最佳SR表现。                                             | ——                                                           | 初始两轮只训练最难的那个任务。                               |
| 2021 | WACV   | [End-to-End Lane Shape Prediction with Transformers](../notes/cvtransformer_lane.md) | 使用transformer自注意力机制对车道线回归多项式建模。捕获车道线全局信息和局部线信息的特征依赖关系，对这些特征关系进行长时建模。 | 主要分为了特征抽取，transformer机制建模，ffns参数输出。      | 在车道线预测上取得了SOTA结果。                               | ——                                                           | transformer简化，不是多头机制，也去除了注意力层的那个全连接。 |
| 2020 | ——     | [Feature Pyramid Transformer](../notes/cvtransformer_fpt.md) |                                                              |                                                              |                                                              |                                                              |                                                              |
| 2021 | ICLR   | [LambdaNetwork](../notes/cvtransformer_lambda.md)            |                                                              |                                                              |                                                              |                                                              |                                                              |

